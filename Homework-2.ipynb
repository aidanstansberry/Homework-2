{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Naive Bayes\n",
    "## Due September 19th\n",
    "\n",
    "## Legal reasoning (From Murphy, 2.2). (25 pts)\n",
    "\n",
    "Suppose a crime has been committed. Blood is found at the scene for which there is no innocent explanation. It is of a type which is present in 1% of the population.  The defendant is known to have this rare blood type.  The  prosecutor claims: “There is a 1% chance that the defendant would have the crime blood type if he were innocent. Thus there is a 99% chance that he guilty”. This is known as the prosecutor’s fallacy. What is wrong with this argument?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "The Probability that the defendent has a given blood type is .01. We also have the information that whoever commited the crime also has this blood type. So the attorney needs to find the probability that the defendent is guilty given that their blood type was found at the crime scene.\n",
    "\n",
    "From Bayes Theorem, \n",
    "$$P(\\mathbf{w}|y) = \\frac{P(y|\\mathbf{w})P(\\mathbf{w})}{P(y)},$$\n",
    "\n",
    "w innocence, and y is the blood type. $P(y) = .01$, that is pretty straight forward, one percent of people have this blood type. For $P(y|w)$ the prosecutor has assumed this to be 1, since there is compelling evidence that a person with this blood type commited the crime. In other words the probability that the defendent has the blood type given that they are guilty is 1. The last number, is the probability that a person is innocent of the crime independent of the test $P(w)$. This number is not well constrained. Plugging in the numbers above and solving for $P(w)$ we find that $P(w)=.0001$ for there to be a 1% chance of innocence given the blood type. This seems quite low all things considered. Essentially the prosecutor has presupposed guilt in making this argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ham vs Spam (75 pts)\n",
    "One use of the naive Bayes classifier, which is still in practical use today, is as a spam filter.  Consider the corpus of text messages packaged with this homework, which are each labelled as either 'spam' or 'ham'.  In this case, naive Bayes utilizes a Bernoulli model that quantifies the probability of a given word given that the message is either spam or ham.  For example, investigating the text messages here, we find that the word *draw* shows up in spam 27 times, yet in ham only 5.  Thus, we have that\n",
    "$$ P(X=\\mathrm{draw}|Y=\\mathrm{ham}) = \\frac{5}{5+27}. $$\n",
    "\n",
    "While this is not particularly strong evidence on its own, we can create a powerful classifier by using the naive assumption in conjunction with all the words in a given message:\n",
    "$$ P(Y=\\mathrm{ham}|X=x) \\propto P(Y=\\mathrm{ham}) \\prod_{i=1}^n P(X_i=x_i|Y=\\mathrm{ham}), $$\n",
    "$$ P(Y=\\mathrm{spam}|X=x) \\propto P(Y=\\mathrm{spam}) \\prod_{i=1}^n P(X_i=x_i|Y=\\mathrm{spam}), $$\n",
    "where $x_i$ are the words in a given message. \n",
    "\n",
    "Your task is to write such a classifier.  I have taken the somewhat tedious step of parsing the data for you, yielding the variables *word_dictionary*, which contains the ham and spam counts for each word, as well as *training_labels*, which provides the spam/ham labels for each text message.  I have also parsed a set of test data: *test_messages* is a list, each entry containing another list of the words in the text message, as well as *test_labels* which contains the spam/ham label for each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Maps from 'ham' or 'spam' strings to zero or one\n",
    "def mapper(s):\n",
    "    if s=='spam':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Read in the text file\n",
    "f = open('SMSSpamCollection','r')\n",
    "lines = f.readlines()\n",
    "# Break out the test data\n",
    "test_lines = lines[:len(lines)//5]\n",
    "lines = lines[len(lines)//5:]\n",
    "\n",
    "# Instantiate the frequency dictionary and an array to\n",
    "# record whether the line is ham or spam\n",
    "word_dictionary = {}\n",
    "training_labels = np.zeros(len(lines),dtype=int)\n",
    "\n",
    "# Loop over all the training messages\n",
    "for i,l in enumerate(lines):\n",
    "    # Split into words\n",
    "    l = l.lower().split()\n",
    "    # Record the special first word which always ham or spam\n",
    "    if l[0]=='ham':\n",
    "        training_labels[i] = 1\n",
    "    # For each word in the message, record whether the message was ham or spam\n",
    "    for w in l[1:]:\n",
    "        # If we've never seen the word before, add a new dictionary entry\n",
    "        if w not in word_dictionary:\n",
    "            word_dictionary[w] = [1,1]\n",
    "        word_dictionary[w][mapper(l[0])] += 1\n",
    "        \n",
    "# Loop over the test messages\n",
    "test_labels = np.zeros(len(test_lines),dtype=int)\n",
    "test_messages = []\n",
    "for i,l in enumerate(test_lines):\n",
    "    l = l.lower().split()\n",
    "    if l[0]=='ham':\n",
    "        test_labels[i] = 1\n",
    "    test_messages.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I have provided code skeletons. Your job is to make the code skeletons into an operational naive Bayes spam detector. (you may discard these skeletons if you would prefer to code this from scratch). Note that lines where you will need to change the code are marked with a '#!'.\n",
    "\n",
    "Your first task is train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67185 25750\n",
      "8.93056485823e-05 3.88349514563e-05\n"
     ]
    }
   ],
   "source": [
    "#What is the prior P(Y=ham) ?\n",
    "ham_prior =  np.sum(training_labels)/float(len(training_labels)) # The sum of the observed hams, \n",
    "#divided by the total number of observations\n",
    "totalham = 0\n",
    "totalspam = 0\n",
    "for key,val in word_dictionary.items(): #finding the total number of spam and ham words\n",
    "    totalham += val[1]\n",
    "    totalspam += val[0]\n",
    "print totalham, totalspam\n",
    "# What are the class probabilities P(X=word|Y=ham) for each word?\n",
    "ham_likelihood = {}\n",
    "spam_likelihood = {}\n",
    "for key,val in word_dictionary.items():\n",
    "    ham_likelihood[key] = float(val[1])/(totalham) #!The mentions in hams, total number of words in ham\n",
    "    spam_likelihood[key] = float(val[0])/(totalspam) #same but with spam values\n",
    "print ham_likelihood['cuz'], spam_likelihood['cuz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to make predictions on a set of test examples which were held back from the training procedure (see *test_messages* variable).  For each of these messages, compute the ham and spam probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:22: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Where to hold the ham and spam posteriors\n",
    "posteriors = np.zeros((len(test_lines),2))\n",
    "# Loop over all the messages in the test set\n",
    "for i,m in enumerate(test_messages):\n",
    "    posterior_ham = 1.0*ham_prior \n",
    "    posterior_spam = 1.0*(1.-ham_prior)\n",
    "    #! Don't forget to include the prior!\n",
    "    # Loop over all the words in each message\n",
    "    for w in m:\n",
    "        # #! What is the purpose of this try/except handler?\n",
    "        # If a word is passed that is not in the dictionary the loop will break,\n",
    "        #so there needs to be a way to avoid words that don't show up in the training set\n",
    "        try:\n",
    "            posterior_ham *= ham_likelihood[w]#! multiply the ham prior by all the probability of a word in ham\n",
    "            posterior_spam *= spam_likelihood[w] #! same but with spam\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    # Notice the normalization factor (denominator) \n",
    "    # to turn these into proper probabilities!\n",
    "    posteriors[i,0] = posterior_spam/(posterior_spam + posterior_ham)\n",
    "    posteriors[i,1] = posterior_ham/(posterior_spam + posterior_ham)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make a ham/spam prediction based on your posterior probabilities.  Compare these to the labels contained in test_labels.  Report the accuracy of your classifier as percentage correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: RuntimeWarning: invalid value encountered in greater\n",
      "  app.launch_new_instance()\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "#print posteriors, test_labels\n",
    "post2 = posteriors\n",
    "post2[(post2>.5)]=1. #converting to boolean arrays, if prob spam > prob ham, it's spam\n",
    "post2[(post2<.5)]=0.\n",
    "post2 = post2[:,1] #the second column is the 0-spam 1-ham column\n",
    "#!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wrong = 14\n",
      "Number of trials = 1114.0\n",
      "My filter is 98.7432675045% accurate.\n"
     ]
    }
   ],
   "source": [
    "trials = float(len(post2)) #number of comparisons in test set\n",
    "total = post2+test_labels #Adding my results to the test labels if the sum is 1,\n",
    "#then my results differ from the test and are incorrect. spam+spam = 0, ham+ham=2, ham+spam=1\n",
    "totalwrong = len(total[total==1.]) # the total number of incorrect responses\n",
    "print 'Total wrong =', totalwrong\n",
    "print 'Number of trials =', trials\n",
    "percentright=(1.-totalwrong/trials)*100 #the percent correct is just 1- the incorrect percent\n",
    "print 'My filter is '+str(percentright)+'% accurate.'  #I'm getting a better accuracy with this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
